# Word2Vec

Word embedding is one of the most popular representations of document vocabulary. It captures the context of a word in a document. In other words, it creates a vectorial representation of each word based on the syntactic and semantic relation that the word has with other words in the same context (next to it).

Word2Vec is one of the most popular technique to learn word embeddings using a shallow neural network.

We implemented the skip-gram version of word2vec with negative sampling.
